export const title = "Dynamic GPU Passthrough on Muxless (or Muxed) Linux Hosts";
export const date = "2025-02-27";
export const excerpt = "Tips on achieving graphics-accelerated VMs on a muxless (or muxed!) NVIDIA PRIME hybrid graphics laptop (or desktop), all without using a silly dummy plug :)";
export const thumbnail = "/assets/img/hybrid-kvm-redmibook.png";

<ImageCarousel
  images={[
    {
      src: "/assets/img/blog-post-imgs/hybrid-graphics-kvm-arch/fastfetch-sysinfo.png",
      alt: "System information output showing specifications of my (muxless) RedmiBook Pro",
      caption: "System specifications from fastfetch"
    },
    {
      src: "/assets/img/blog-post-imgs/hybrid-graphics-kvm-arch/hybrid-kvm-redmibook.png",
      alt: "Windows 10 virtual machine running with NVIDIA graphics acceleration on the muxless Redmibook!",
      caption: "Windows 10 virtual machine running with NVIDIA graphics acceleration on the muxless Redmibook!"
    }
  ]}
/>
### Laptop Specifications

- Model: RedmiBook Pro 15 (2021)
- CPU: Intel i7-11370H (4 cores, 8 threads)
- dGPU: NVIDIA MX450, 2GB VRAM
- iGPU: Iris Xe Graphics (TGL2)
- RAM: 16GB 3200MHz Soldered RAM :(
- Display: 15.6" 3200x2000 90Hz IPS (!!!)

### Preamble

Linux is great, but `${CURRENT_YEAR}` is still not quite the year of Linux on the desktop (or laptop, for that matter). Lots of software and games will simply never get ported to Linux, and sometimes frameworks that aim to run Windows programs on Linux like Wine and Proton aren't good enough for some software yet...

Thus, KVMs (kernel-based virtual machines) for Windows with GPU passthrough are desirable and oftentimes a necessity for many Linux users.

One of the biggest difficulties when setting up a Windows KVM on a hybrid graphics machines is actually being able to SEE the graphically-accelerated output from the passed-through dGPU through the host.

### Requirements/Software Used

- A laptop with at least two GPUs (iGPU and dGPU)
- Linux software:
  - I used Arch (btw) but the same general setup should work across distros?
  - libvirtd, virt-manager, qemu, etc.
  - [looking-glass-module-dkms](https://aur.archlinux.org/packages/looking-glass-module-dkms)
- Windows VM software:
  - looking-glass-host
  - Virtual-Display-Driver

### Getting our (Arch) Linux host ready

> **NOTE:** These steps are largely based off the instructions outlined in the [PCI passthrough via OVMF](https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Binding_vfio-pci_via_device_ID) page on the Arch Linux Wiki, as well as [this great QEMU-KVM setup guide for Arch Linux](https://gist.github.com/tatumroaquin/c6464e1ccaef40fd098a4f31db61ab22) by [tatumroaquin](https://gist.github.com/tatumroaquin)!

1. [Verify that IOMMU is enabled](https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Setting_up_IOMMU)
2. [Ensure your IOMMU groups are valid](https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Ensuring_that_the_groups_are_valid)
3. [Install the necessary packages](https://gist.github.com/tatumroaquin/c6464e1ccaef40fd098a4f31db61ab22#install-qemu-libvirt-viewers-and-tools) (`sudo pacman -S qemu-desktop libvirt dnsmasq swtpm edk2-ovmf`)
4. Add your user to the libvirt group (`sudo usermod -a -G libvirt $USER`)
5. Uncomment and modify lines in `/etc/libvirt/qemu.conf`:

<details>
<summary>Example <code>/etc/libvirt/qemu.conf</code> changes</summary>

```sh
sudo nano /etc/libvirt/qemu.conf
```
```conf
# MODIFY THE LINE BELOW TO INCLUDE YOUR $USER
user = "nick" # MODIFY THIS LINE TO INCLUDE YOUR $USER

# UNCOMMENT THE BLOCK BELOW, ADD "/dev/kvmfr0" TO THE ARRAY LIKE SO!
cgroup_device_acl = [
    "/dev/null", "/dev/full", "/dev/zero",
    "/dev/random", "/dev/urandom",
    "/dev/ptmx", "/dev/kvm",
    "/dev/userfaultfd", "/dev/kvmfr0"
]
# UNCOMMENT THE BLOCK ABOVE, ADD "/dev/kvmfr0" TO THE ARRAY LIKE SO!
```
</details>

6. Enable & start libvirtd.service:  
   `sudo systemctl enable --now libvirtd.service`
7. Start the libvirt network:  
   `sudo virsh net-autostart default`
8. Create `/etc/modules-load.d/kvmfr.conf` to load the `kvmfr` module for Looking-Glass on boot:  
   `sudo nano /etc/modules-load.d/kvmfr.conf`  
   Contents:
   ```
   # KVMFR Looking Glass module
   kvmfr
   ```
9. Create `/etc/modprobe.d/kvmfr.conf` to set the framebuffer size on boot (Calculate framebuffer size [here](https://looking-glass.io/docs/B7/install_libvirt/#determining-memory)):  
   `sudo nano /etc/modprobe.d/kvmfr.conf`  
   Contents:
   ```
   options kvmfr static_size_mb=64
   ```
10. Create `99-kvmfr.rules` udev rule to correct kvmfr0 permissions:  
    `sudo nano /etc/udev/rules.d/99-kvmfr.rules`  
    Contents:
    ```
    # REPLACE $USER WITH YOUR USERNAME!!
    SUBSYSTEM=="kvmfr", OWNER="$USER", GROUP="kvm", MODE="0660"
    ```
11. Run the following or just rebuild initramfs & reboot:  
    `sudo modprobe kvmfr static_size_mb=64`
12. Create Windows VM using `virt-manager`, but don't attach the dGPU yet!

### Setting up dynamically unbinding the NVIDIA GPU from the Linux host

[Set up QEMU hooks](https://passthroughpo.st/simple-per-vm-libvirt-hooks-with-the-vfio-tools-hook-helper/) by simply creating the `hooks` directory and running the script provided by PassthroughPOST:

```sh
sudo mkdir -p /etc/libvirt/hooks
sudo wget 'https://raw.githubusercontent.com/PassthroughPOST/VFIO-Tools/master/libvirt_hooks/qemu' -O /etc/libvirt/hooks/qemu
sudo chmod +x /etc/libvirt/hooks/qemu
```

Then, create a folder at `/etc/libvirt/hooks/qemu.d/` with the name of the VM you created earlier.

For example, let's say you named your VM `Windows-11`:

```bash
sudo mkdir -p /etc/libvirt/hooks/qemu.d/Windows-11/
```

Create the folders for the begin and end hooks:

```bash
sudo mkdir -p /etc/libvirt/hooks/qemu.d/${VMNAME}/prepare/begin
sudo mkdir -p /etc/libvirt/hooks/qemu.d/${VMNAME}/release/end
```

Finally, we can create our start and stop scripts:

**prepare/begin/start.sh**
```sh
#!/bin/bash
# Unload NVIDIA Modules
modprobe -r nvidia_uvm nvidia_drm nvidia_modeset nvidia
# stop nvidia-persistenced
systemctl stop nvidia-persistenced.service
```

**release/end/stop.sh**
```sh
#!/bin/bash
# Re-load NVIDIA Modules
modprobe nvidia
sleep 1
modprobe nvidia_modeset
sleep 1
modprobe nvidia_uvm
sleep 1
modprobe nvidia_drm
sleep 1
# (re)start nvidia-persistenced
systemctl start nvidia-persistenced
```

Make these scripts executable:

```sh
chmod +x /etc/libvirt/hooks/qemu.d/${VMNAME}/prepare/begin/start.sh
chmod +x /etc/libvirt/hooks/qemu.d/${VMNAME}/release/end/stop.sh
```

Restart libvirtd:

```sh
sudo systemctl restart libvirtd
```

Add the NVIDIA dGPU PCI device(s) to your VM using `virt-manager`!

Starting the VM with these scripts should successfully 'unload' the NVIDIA GPU from Linux and 'give it' to the Windows VM!
No need to touch `vfio_pci` drivers... :)

## Getting accelerated video output through Looking-Glass without a physical dummy plug using Virtual Display Driver (VDD)

Once your Windows VM can boot with the NVIDIA card attached, use the SPICE/VGA video passthrough through `virt-manager` to complete the Windows setup and install the NVIDIA graphics drivers.

Once the NVIDIA drivers are installed, download and install the latest releases of [Looking-Glass-Host](https://looking-glass.io/downloads) and [Virtual-Display-Driver](https://github.com/VirtualDisplay/Virtual-Display-Driver/releases/latest).

Here's the weird part. In my experience, simply removing the SPICE video devices from the VM via `virt-manager` would result in Looking-Glass giving a blank/black picture, regardless of which display was set default in Windows, etc.

I worked around this by doing the following, in this order:
- Right clicked the desktop to get to Display Settings (or go to Windows Settings > System > Display Settings)
- Determined which output/screen index corresponded to either SPICE or the Virtual Display by going to 'Advanced Display' ('Wired Monitor' is usually SPICE, Virtual Display Driver should be obvious by its name)
- For this example, we will assume that SPICE = Display 1, VDD = Display 2.
- Select the option to show only on the Virtual Display Device. So, if VDD is Display 2, select 'Show only on 2'.
- Both screens (Looking-Glass AND SPICE via `virt-manager`) SHOULD BE BLACK.
- QUICKLY* turn off the VM via `virt-manager` (through the host), and turn it back on once it's powered off fully. (*do it within ~15 seconds to prevent Windows from undoing the changes)
- Upon the next boot, Looking-Glass should be the only 'display' active. This display should have functioning graphics acceleration via the NVIDIA card unlike the regular old SPICE window, all without a physical dummy connector!

There's probably a more elegant way of disabling the SPICE monitor to enable proper output via Looking-Glasss, but this worked for me.

### WHY are we doing this?

The most common solution in the past was to use a HDMI/DisplayPort dummy display plug plugged into one of the laptop's dGPU outputs, 'simulating' a display for the passed-through dGPU to render to, alongside Looking-Glass to then mirror that accelerated 'display' from the guest to the host's internal display on Linux through the Looking-Glass client. Another solution was to simply just not use the internal display for KVMs when doing accelerated passthrough at all and to use an external display plugged into the laptop for KVMs.

These solutions work, but... they either require the purchase & constant use of a physical dummy plug, or the reliance on an external monitor when using our Windows KVM. There's nothing wrong with this, but these solutions aren't quite 'elegant' and do not work for muxless laptops.

### Muxless? Muxed?

The laptop I use to demonstrate this is *muxless*, meaning that ALL graphics/video/display shown on the internal and external displays are 'presented' using the iGPU. That means that even if the NVIDIA dGPU is rendering something, the NVIDIA dGPU 'relies' on the Intel iGPU to actually 'show' what it's doing.

A *muxed* laptop, on the other hand, supports dynamic switching between the iGPU/dGPU for the internal and external displays. Muxed laptops are often regarded as being 'better' for KVMs and GPU passthrough vs. muxless laptops, however, in my experience, both setups have very similar difficulties that are solved largely in the same ways, and neither is 'objectively better' for the purposes of running a graphically-accelerated KVM (in my experience).

PRACTICALLY SPEAKING... the 'method'/strategy we will be using in this guide to get graphically accelerated output from our Windows KVM should work whether you have a muxed or muxless laptop!

### Solving `modprobe: FATAL: Module nvidia_drm is in use`

#### PROBLEM:
Some applications use `nvidia_drm` unexpectedly, therefore not allowing the `start.sh` QEMU hook to run correctly. This problem could be a symptom of another issue with my configuration and could possibly be solved a 'better' way, but this solution works well enough for me:

```sh
sudo modprobe -r nvidia_drm
# modprobe: FATAL: Module nvidia_drm is in use.
```

Running `sudo lsmod | grep nvidia` revealed that 2 processes that were using `nvidia_drm`:

```sh
sudo lsmod | grep -iE nvidia
nvidia_drm            139264  2
nvidia_modeset       1830912  1 nvidia_drm
nvidia              96956416  6 nvidia_modeset
drm_ttm_helper         16384  2 nvidia_drm,xe
video                  81920  3 xe,i915,nvidia_modeset
```

Running `sudo lsof | grep -iE nvidia` gave me the list/history of processes that have run on the NVIDIA driver. I saw a whole bunch of entries for `electron`. I closed all Electron apps running (Discord, Mullvad VPN, Beeper etc.) and re-ran `sudo lsmod | grep nvidia`:

```sh
nick@hybrid:~$ sudo lsmod | grep -iE nvidia
nvidia_uvm           3977216  0
nvidia_drm            139264  0
nvidia_modeset       1830912  1 nvidia_drm
nvidia              96956416  2 nvidia_uvm,nvidia_modeset
drm_ttm_helper         16384  2 nvidia_drm,xe
video                  81920  3 xe,i915,nvidia_modeset
```

Now `nvidia_drm` is being used by 0 processes, so we can now unload `nvidia` drivers successfully:

```sh
nick@hybrid:~$ sudo modprobe -r nvidia_uvm nvidia_drm nvidia_modeset nvidia 

nick@hybrid:~$ 
```

My solution is kinda lazy, but works well enough for me. For each application that was using `nvidia_drm`, I added `VK_ICD_FILENAMES=/usr/share/vulkan/icd.d/intel_icd.x86_64.json` to the `Environment variables` of Discord*, Mullvad VPN and Beeper. For more 'stubbourn' programs, I simply add a `pkill` line for that program to the `start.sh` script of my VM, and you can even automatically restart the `pkill`'d application using another QEMU hook!

Contents of `/etc.modprobe.d/nvidia.conf`:
```conf
options nvidia "NVreg_EnableGpuFirmware=0"
```

These options disable (part of?) the NVIDIA GPU firmware? Without this option, the card would stay in D0 and never enter D3cold.

```sh
nick@hybrid:~$ sudo cat /proc/driver/nvidia/gpus/0000:2b:00.0/power
Runtime D3 status:          Enabled (fine-grained)
Video Memory:               Off

GPU Hardware Support:
 Video Memory Self Refresh: Supported
 Video Memory Off:          Supported

S0ix Power Management:
 Platform Support:          Supported
 Status:                    Disabled

Notebook Dynamic Boost:     Not Supported